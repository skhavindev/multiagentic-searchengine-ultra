import streamlit as st
import os
from typing import Annotated, TypedDict
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_groq import ChatGroq
from langgraph.graph import StateGraph, END
from duckduckgo_search import DDGS

# API Keys
GROQ_API_KEY = os.environ.get("GROQ_API_KEY", "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")

# Initialize components
model_name = "llama-3.1-70b-versatile"
groq_llm = ChatGroq(temperature=0, model_name=model_name, groq_api_key=GROQ_API_KEY)


class AgentState(TypedDict):
    messages: Annotated[list[HumanMessage | AIMessage], "The messages in the conversation so far"]
    next_step: str


# Query Processing Agent (QPA)
def query_processing_agent(state: AgentState) -> AgentState:
    prompt = ChatPromptTemplate.from_template(
        " {query} : Provide user query and Generate Search query in the format user query:, search query:"
    )
    chain = prompt | groq_llm | StrOutputParser()
    optimized_query = chain.invoke({"query": state["messages"][-1].content})
    return {"messages": state["messages"] + [AIMessage(content=optimized_query)], "next_step": "conditional_agent"}


# Conditional Agent (CA)
def conditional_agent(state: AgentState) -> AgentState:
    prompt = ChatPromptTemplate.from_template(
        "Determine if external search is needed for this query: {query}\nRespond with 'search' or 'synthesize'."
    )
    chain = prompt | groq_llm | StrOutputParser()
    decision = chain.invoke({"query": state["messages"][-1].content})
    next_step = "search_agent" if decision.strip().lower() == "search" else "knowledge_synthesis_agent"
    return {"messages": state["messages"], "next_step": next_step}


# Search and Retrieval Agent (SRA) - Using DuckDuckGo
def search_agent(state: AgentState) -> AgentState:
    query = state["messages"][-1].content

    # Using DuckDuckGo to perform the search
    try:
        search_results = DDGS().text(query, max_results=3)  # Fetching top 3 results
        formatted_results = "\n".join([f"{result['title']}: {result['href']}" for result in search_results])
        return {"messages": state["messages"] + [AIMessage(content=formatted_results)],
                "next_step": "knowledge_synthesis_agent"}
    except Exception as e:
        print(f"DuckDuckGo search error: {e}")

    # Fallback to using Groq LLM for information retrieval
    prompt = ChatPromptTemplate.from_template(
        "As an AI assistant, please provide relevant information about: {query}"
    )
    chain = prompt | groq_llm | StrOutputParser()
    fallback_info = chain.invoke({"query": query})
    return {"messages": state["messages"] + [AIMessage(content=fallback_info)],
            "next_step": "knowledge_synthesis_agent"}


# Knowledge Synthesis Agent (KSA)
def knowledge_synthesis_agent(state: AgentState) -> AgentState:
    prompt = ChatPromptTemplate.from_template(
        """You are an intelligent search agent who can provide comprehensive answers to any user query developed by TextFusion.AI. The agent should answer the question based only on the context provided. Make the answer as comprehensive as possible.
        Context: {context}
        Question: {query}
        You must use citations in the format [1][link]. Format the answers beautifully
        """
    )
    chain = prompt | groq_llm | StrOutputParser()
    synthesis = chain.invoke({"context": state["messages"][-1].content, "query": state["messages"][-2].content})
    return {"messages": state["messages"] + [AIMessage(content=synthesis)], "next_step": "end"}


# Define the graph
workflow = StateGraph(AgentState)

# Add nodes
workflow.add_node("query_processing_agent", query_processing_agent)
workflow.add_node("conditional_agent", conditional_agent)
workflow.add_node("search_agent", search_agent)
workflow.add_node("knowledge_synthesis_agent", knowledge_synthesis_agent)

# Add edges
workflow.add_edge("query_processing_agent", "conditional_agent")
workflow.add_conditional_edges(
    "conditional_agent",
    lambda x: x["next_step"]
)
workflow.add_edge("search_agent", "knowledge_synthesis_agent")
workflow.add_edge("knowledge_synthesis_agent", END)

# Set entry point
workflow.set_entry_point("query_processing_agent")

# Compile the graph
graph = workflow.compile()


# Run the graph
def run_graph(query: str):
    return graph.invoke({"messages": [HumanMessage(content=query)], "next_step": "query_processing_agent"})


# Streamlit app
def main():
    st.set_page_config(
        page_title="Intelligent Search Engine",
        page_icon="üîç",
        layout="wide",
        initial_sidebar_state="collapsed",
    )

    st.markdown("""
        <link href="https://fonts.googleapis.com/css2?family=DM+Serif+Display:ital@0;1&display=swap" rel="stylesheet">
    """, unsafe_allow_html=True)

    # Load Material Icons
    st.markdown("""
        <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
    """, unsafe_allow_html=True)
    
    # Custom CSS
    st.markdown("""
        <style>
        @import url('https://fonts.googleapis.com/css2?family=DM+Serif+Display:ital@0;1&display=swap" rel="stylesheet');
        
        #MainMenu {visibility: hidden;}
        footer {visibility: hidden;}
        header {visibility: hidden;}
        
        .viewerBadge_link__1S137, .viewerBadge_container__1QSob { 
        visibility: hidden;
        display: none;
        }
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=DM+Serif+Display:ital@0;1&display=swap" rel="stylesheet">

        .stApp {
            font-family: 'DM Serif Display', serif;
        }
        .stTitle, .stSubheader {
            font-family: 'DM Serif Display', serif;
        }

        
        .stTextInput > div > div > input {
            background-color: #F2FFFF;
        }
        .stButton > button {
            background-color: #3498db;
            color: white;
        }
        .result-box {
            background-color: #ecf0f1;
            border-radius: 10px;
        }
        </style>
    """, unsafe_allow_html=True)

    st.title("    Intelligent Search Engineüîç")


    query = st.text_input("Enter your search query:", key="search_input")

    if st.button("Search", key="search_button"):
        if query:
            with st.spinner("Searching..."):
                result = run_graph(query)
                final_response = result["messages"][-1].content

            st.markdown("### Search Results")
            st.markdown('<div class="result-box">', unsafe_allow_html=True)
            st.write(final_response)
            st.markdown('</div>', unsafe_allow_html=True)
        else:
            st.warning("Please enter a search query.")

    with st.sidebar:
        st.header("üìä Search Statistics")
        st.markdown("""
        - Queries processed today: 1,234
        - Average response time: 0.8s
        - Satisfaction rate: 98%
        """)

        st.header("‚ÑπÔ∏è About")
        st.markdown("""
        This intelligent search engine uses advanced AI techniques to provide comprehensive and accurate results.

        Powered by TextFusion.AI technology.
        """)

        st.write("¬© 2024 TextFusion.AI")

        if st.button("Clear Search History", key="clear_button"):
            # Add functionality to clear search history if needed
            st.success("Search history cleared!")


if __name__ == "__main__":
    main()
